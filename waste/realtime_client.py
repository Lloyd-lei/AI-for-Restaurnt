#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
OpenAI Realtime API å®¢æˆ·ç«¯ - æ”¯æŒè¯­éŸ³æ‰“æ–­åŠŸèƒ½
"""

import asyncio
import websockets
import json
import pyaudio
import base64
import os
from dotenv import load_dotenv
import signal
import sys
import struct
import math

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

# é…ç½®å‚æ•°
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
REALTIME_API_URL = "wss://api.openai.com/v1/realtime"
MODEL = "gpt-4o-realtime-preview-2024-10-01"

# éŸ³é¢‘å‚æ•°ï¼ˆOpenAI Realtime API è¦æ±‚ï¼‰
SAMPLE_RATE = 24000  # 24kHz
CHANNELS = 1  # å•å£°é“ï¼ˆOpenAI è¦æ±‚ï¼‰
CHUNK_SIZE = 4800  # 200ms çš„éŸ³é¢‘å— (24000 * 0.2)
FORMAT = pyaudio.paInt16  # 16-bit PCM

# å›éŸ³æ¶ˆé™¤å‚æ•°
INTERRUPT_THRESHOLD = 0  # æ‰“æ–­é˜ˆå€¼ï¼ˆéŸ³é¢‘èƒ½é‡ï¼‰ï¼Œè¶Šå¤§è¶Šä¸å®¹æ˜“è§¦å‘æ‰“æ–­
ENABLE_ECHO_CANCELLATION = False  # æ˜¯å¦å¯ç”¨å›éŸ³æ¶ˆé™¤ï¼ˆAIæ’­æ”¾æ—¶ä¸å‘é€éº¦å…‹é£æ•°æ®ï¼‰

# æ’­æ”¾é€Ÿåº¦å‚æ•°
PLAYBACK_SPEED = 1  # æ’­æ”¾é€Ÿåº¦å€æ•°ï¼ˆ1.0 = æ­£å¸¸ï¼Œ1.2 = 1.2å€é€Ÿï¼Œå»ºè®®èŒƒå›´ 1.0-1.5ï¼‰

# éŸ³é¢‘é¢„å¤„ç†å‚æ•°
ENABLE_GAIN_CONTROL = False  # æ˜¯å¦å¯ç”¨å¢ç›Šæ§åˆ¶
TARGET_GAIN = 1  # ç›®æ ‡å¢ç›Šå€æ•°ï¼ˆå»ºè®® 1.0-3.0ï¼‰
ENABLE_NOISE_GATE = False  # æ˜¯å¦å¯ç”¨å™ªå£°é—¨ï¼ˆè¿‡æ»¤ä½èƒ½é‡å™ªéŸ³ï¼‰
NOISE_GATE_THRESHOLD = 30  # å™ªå£°é—¨é˜ˆå€¼ï¼Œä½äºæ­¤å€¼çš„éŸ³é¢‘ä¼šè¢«é™éŸ³

def calculate_audio_energy(audio_data):
    """è®¡ç®—éŸ³é¢‘èƒ½é‡ï¼ˆRMS - Root Mean Squareï¼‰"""
    # å°†å­—èŠ‚æ•°æ®è½¬æ¢ä¸º 16-bit æ•´æ•°
    samples = struct.unpack(f'{len(audio_data)//2}h', audio_data)
    # è®¡ç®— RMS
    sum_squares = sum(sample ** 2 for sample in samples)
    rms = math.sqrt(sum_squares / len(samples))
    return int(rms)

def apply_gain(audio_data, gain=1.5):
    """åº”ç”¨å¢ç›Šï¼ˆæ”¾å¤§éŸ³é¢‘ï¼‰"""
    samples = struct.unpack(f'{len(audio_data)//2}h', audio_data)
    # åº”ç”¨å¢ç›Šå¹¶é™åˆ¶åœ¨ int16 èŒƒå›´å†…
    amplified = [max(-32768, min(32767, int(sample * gain))) for sample in samples]
    return struct.pack(f'{len(amplified)}h', *amplified)

def apply_noise_gate(audio_data, threshold=30):
    """åº”ç”¨å™ªå£°é—¨ï¼ˆè¿‡æ»¤ä½èƒ½é‡éŸ³é¢‘ï¼‰"""
    energy = calculate_audio_energy(audio_data)
    if energy < threshold:
        # è¿”å›é™éŸ³
        return b'\x00' * len(audio_data)
    return audio_data

def preprocess_audio(audio_data):
    """
    éŸ³é¢‘é¢„å¤„ç†ä¸»å‡½æ•°
    åŒ…æ‹¬ï¼šå™ªå£°é—¨ã€å¢ç›Šæ§åˆ¶ç­‰
    """
    processed = audio_data
    
    # 1. å™ªå£°é—¨ï¼ˆè¿‡æ»¤ç¯å¢ƒå™ªéŸ³ï¼‰
    if ENABLE_NOISE_GATE:
        processed = apply_noise_gate(processed, NOISE_GATE_THRESHOLD)
    
    # 2. å¢ç›Šæ§åˆ¶ï¼ˆå¢å¼ºéŸ³é‡ï¼‰
    if ENABLE_GAIN_CONTROL:
        processed = apply_gain(processed, TARGET_GAIN)
    
    return processed

class RealtimeClient:
    def __init__(self):
        self.ws = None
        self.audio = pyaudio.PyAudio()
        self.input_stream = None
        self.output_stream = None
        self.is_running = False
        self.is_ai_speaking = False
        self.session_id = None
        self.cancel_sent = False  # é˜²æ­¢é‡å¤å‘é€å–æ¶ˆæ¶ˆæ¯
        
    async def connect(self):
        """è¿æ¥åˆ° OpenAI Realtime API"""
        headers = {
            "Authorization": f"Bearer {OPENAI_API_KEY}",
            "OpenAI-Beta": "realtime=v1"
        }
        
        url = f"{REALTIME_API_URL}?model={MODEL}"
        
        print("ğŸ”„ æ­£åœ¨è¿æ¥åˆ° OpenAI Realtime API...")
        self.ws = await websockets.connect(url, extra_headers=headers)
        print("âœ… å·²è¿æ¥åˆ° OpenAI Realtime API")
        
        # é…ç½®ä¼šè¯
        await self.configure_session()
        
    async def configure_session(self):
        """é…ç½®ä¼šè¯å‚æ•°"""
        config = {
            "type": "session.update",
            "session": {
                "modalities": ["text", "audio"],
                "instructions": (
                    "ä½ æ˜¯ä¸€ä¸ªå‹å¥½çš„å¤šè¯­è¨€AIåŠ©æ‰‹ã€‚è¯·éµå¾ªä»¥ä¸‹è§„åˆ™ï¼š\n"
                    "1. è‡ªåŠ¨æ£€æµ‹ç”¨æˆ·ä½¿ç”¨çš„è¯­è¨€\n"
                    "2. ç”¨æˆ·è¯´ä»€ä¹ˆè¯­è¨€ï¼Œä½ å°±ç”¨ä»€ä¹ˆè¯­è¨€å›å¤\n"
                    "3. å¦‚æœç”¨æˆ·è¯´ä¸­æ–‡ï¼Œä½ å°±ç”¨ä¸­æ–‡å›å¤\n"
                    "4. å¦‚æœç”¨æˆ·è¯´è‹±è¯­ï¼Œä½ å°±ç”¨è‹±è¯­å›å¤\n"
                    "5. å¦‚æœç”¨æˆ·è¯´æ—¥è¯­ï¼Œä½ å°±ç”¨æ—¥è¯­å›å¤\n"
                    "6. ä¿æŒç®€æ´ã€è‡ªç„¶çš„å¯¹è¯é£æ ¼\n"
                    "7. ä¸è¦æ··ç”¨å¤šç§è¯­è¨€ï¼Œå§‹ç»ˆä½¿ç”¨ç”¨æˆ·å½“å‰ä½¿ç”¨çš„è¯­è¨€"
                ),
                "voice": "shimmer",  # shimmer å£°éŸ³ï¼ˆå¥³å£°ï¼Œæ¸©æš–å‹å¥½ï¼‰
                "input_audio_format": "pcm16",
                "output_audio_format": "pcm16",
                "input_audio_transcription": {
                    "model": "whisper-1"
                },
                "turn_detection": {
                    "type": "server_vad",  # æœåŠ¡å™¨ç«¯è¯­éŸ³æ´»åŠ¨æ£€æµ‹
                    "threshold": 0.5,
                    "prefix_padding_ms": 300,
                    "silence_duration_ms": 500  # 500ms é™éŸ³åç»“æŸ
                }
            }
        }
        
        await self.ws.send(json.dumps(config))
        print("âš™ï¸  ä¼šè¯é…ç½®å·²å‘é€ï¼ˆå¤šè¯­è¨€æ¨¡å¼ + shimmer å£°éŸ³ï¼‰")
        
    async def start_audio_input(self):
        """å¯åŠ¨éº¦å…‹é£éŸ³é¢‘è¾“å…¥"""
        self.input_stream = self.audio.open(
            format=FORMAT,
            channels=CHANNELS,
            rate=SAMPLE_RATE,
            input=True,
            frames_per_buffer=CHUNK_SIZE
        )
        
        print("ğŸ™ï¸  éº¦å…‹é£å·²å¯åŠ¨ï¼Œå¼€å§‹ç›‘å¬...")
        
        while self.is_running:
            try:
                # åœ¨çº¿ç¨‹ä¸­è¯»å–éŸ³é¢‘æ•°æ®ï¼ˆé¿å…é˜»å¡äº‹ä»¶å¾ªç¯ï¼‰
                audio_data = await asyncio.to_thread(
                    self.input_stream.read, CHUNK_SIZE, False
                )
                
                # æ£€æŸ¥ WebSocket è¿æ¥çŠ¶æ€
                if not self.ws or self.ws.closed:
                    print("âŒ WebSocket è¿æ¥å·²å…³é—­")
                    break
                
                # ğŸ”‡ å¦‚æœ AI æ­£åœ¨æ’­æ”¾è¯­éŸ³ï¼Œä¸å‘é€éº¦å…‹é£æ•°æ®ï¼ˆé˜²æ­¢å›éŸ³ï¼‰
                if self.is_ai_speaking and ENABLE_ECHO_CANCELLATION:
                    # æ£€æŸ¥éŸ³é¢‘èƒ½é‡ï¼Œåˆ¤æ–­ç”¨æˆ·æ˜¯å¦åœ¨è¯´è¯ï¼ˆç®€å•çš„ VADï¼‰
                    audio_level = calculate_audio_energy(audio_data)
                    
                    # å¦‚æœéŸ³é¢‘èƒ½é‡è¶…è¿‡é˜ˆå€¼ï¼Œè¯´æ˜ç”¨æˆ·åœ¨è¯´è¯ï¼Œè§¦å‘æ‰“æ–­
                    if audio_level > INTERRUPT_THRESHOLD and not self.cancel_sent:
                        print(f"âš¡ æ£€æµ‹åˆ°ç”¨æˆ·è¯´è¯ï¼ˆèƒ½é‡: {audio_level}ï¼‰ï¼Œæ‰“æ–­AIå›å¤")
                        # å‘é€å–æ¶ˆå“åº”çš„æ¶ˆæ¯
                        try:
                            await self.ws.send(json.dumps({
                                "type": "response.cancel"
                            }))
                            self.cancel_sent = True
                            self.is_ai_speaking = False
                        except Exception as e:
                            print(f"âŒ å‘é€å–æ¶ˆæ¶ˆæ¯å¤±è´¥: {e}")
                    else:
                        # AI åœ¨è¯´è¯ä¸”ç”¨æˆ·æ²¡æœ‰æ‰“æ–­ï¼Œè·³è¿‡è¿™å¸§ï¼ˆé˜²æ­¢å›éŸ³ï¼‰
                        continue
                
                # ğŸ›ï¸ éŸ³é¢‘é¢„å¤„ç†ï¼ˆé™å™ªã€å¢ç›Šï¼‰
                processed_audio = preprocess_audio(audio_data)
                
                # å°†éŸ³é¢‘ç¼–ç ä¸º base64 å¹¶å‘é€
                audio_b64 = base64.b64encode(processed_audio).decode('utf-8')
                
                message = {
                    "type": "input_audio_buffer.append",
                    "audio": audio_b64
                }
                
                try:
                    await self.ws.send(json.dumps(message))
                except Exception as e:
                    if self.is_running:
                        print(f"âŒ å‘é€éŸ³é¢‘æ•°æ®å¤±è´¥: {e}")
                        break
                
                # çŸ­æš‚å»¶è¿Ÿï¼Œé¿å…å‘é€è¿‡å¿«
                await asyncio.sleep(0.01)
                
            except Exception as e:
                if self.is_running:
                    print(f"âŒ éŸ³é¢‘è¾“å…¥é”™è¯¯: {e}")
                break
                
    async def start_audio_output(self):
        """å¯åŠ¨éŸ³é¢‘è¾“å‡º"""
        # é€šè¿‡è°ƒæ•´æ’­æ”¾é‡‡æ ·ç‡æ¥æ”¹å˜æ’­æ”¾é€Ÿåº¦
        playback_rate = int(SAMPLE_RATE * PLAYBACK_SPEED)
        
        self.output_stream = self.audio.open(
            format=FORMAT,
            channels=CHANNELS,
            rate=playback_rate,  # ä½¿ç”¨è°ƒæ•´åçš„é‡‡æ ·ç‡
            output=True,
            frames_per_buffer=CHUNK_SIZE
        )
        
        if PLAYBACK_SPEED != 1.0:
            print(f"ğŸ”Š éŸ³é¢‘è¾“å‡ºå·²å¯åŠ¨ï¼ˆ{PLAYBACK_SPEED}x å€é€Ÿæ’­æ”¾ï¼‰")
        else:
            print("ğŸ”Š éŸ³é¢‘è¾“å‡ºå·²å¯åŠ¨")
        
    async def handle_messages(self):
        """å¤„ç†æ¥è‡ªæœåŠ¡å™¨çš„æ¶ˆæ¯"""
        try:
            async for message in self.ws:
                data = json.loads(message)
                event_type = data.get("type")
                
                # ä¼šè¯åˆ›å»º
                if event_type == "session.created":
                    self.session_id = data.get("session", {}).get("id")
                    print(f"ğŸ“ ä¼šè¯å·²åˆ›å»º: {self.session_id}")
                
                # ä¼šè¯æ›´æ–°
                elif event_type == "session.updated":
                    print("âœ… ä¼šè¯é…ç½®å·²æ›´æ–°")
                
                # è¾“å…¥éŸ³é¢‘ç¼“å†²åŒºå·²æäº¤
                elif event_type == "input_audio_buffer.committed":
                    print("ğŸ¤ ç”¨æˆ·è¯­éŸ³å·²æäº¤")
                
                # å¯¹è¯åˆ›å»º
                elif event_type == "conversation.item.created":
                    item = data.get("item", {})
                    if item.get("role") == "user":
                        print("ğŸ‘¤ ç”¨æˆ·æ¶ˆæ¯å·²åˆ›å»º")
                
                # å“åº”å¼€å§‹
                elif event_type == "response.created":
                    print("ğŸ¤– AI å¼€å§‹ç”Ÿæˆå›å¤...")
                    self.cancel_sent = False  # é‡ç½®å–æ¶ˆæ ‡å¿—
                
                # å“åº”å®Œæˆ
                elif event_type == "response.done":
                    print("âœ… AI å›å¤å®Œæˆ")
                    self.is_ai_speaking = False
                    self.cancel_sent = False
                
                # éŸ³é¢‘è½¬å½•ï¼ˆç”¨æˆ·è¯´çš„è¯ï¼‰
                elif event_type == "conversation.item.input_audio_transcription.completed":
                    transcript = data.get("transcript", "")
                    print(f"ğŸ“ ä½ è¯´: {transcript}")
                
                # AI æ–‡æœ¬å›å¤
                elif event_type == "response.text.delta":
                    delta = data.get("delta", "")
                    print(delta, end="", flush=True)
                
                elif event_type == "response.text.done":
                    text = data.get("text", "")
                    if text:
                        print(f"\nğŸ’¬ AI å›å¤: {text}")
                
                # éŸ³é¢‘æ•°æ®
                elif event_type == "response.audio.delta":
                    self.is_ai_speaking = True
                    audio_b64 = data.get("delta", "")
                    if audio_b64:
                        # è§£ç å¹¶æ’­æ”¾éŸ³é¢‘
                        audio_data = base64.b64decode(audio_b64)
                        if self.output_stream:
                            self.output_stream.write(audio_data)
                
                elif event_type == "response.audio.done":
                    print("ğŸ”Š AI è¯­éŸ³æ’­æ”¾å®Œæˆ")
                    self.is_ai_speaking = False
                
                # å“åº”è¢«å–æ¶ˆï¼ˆæ‰“æ–­ï¼‰
                elif event_type == "response.cancelled":
                    print("âš¡ AI å›å¤å·²è¢«æ‰“æ–­")
                    self.is_ai_speaking = False
                    self.cancel_sent = False
                
                # é”™è¯¯å¤„ç†
                elif event_type == "error":
                    error = data.get("error", {})
                    print(f"âŒ é”™è¯¯: {error.get('message', 'æœªçŸ¥é”™è¯¯')}")
                
        except websockets.exceptions.ConnectionClosed:
            print("ğŸ”Œ è¿æ¥å·²å…³é—­")
        except Exception as e:
            print(f"âŒ æ¶ˆæ¯å¤„ç†é”™è¯¯: {e}")
            
    async def run(self):
        """è¿è¡Œå®¢æˆ·ç«¯"""
        try:
            # è¿æ¥åˆ°æœåŠ¡å™¨
            await self.connect()
            
            # å¯åŠ¨éŸ³é¢‘è¾“å‡º
            await self.start_audio_output()
            
            # è®¾ç½®è¿è¡Œæ ‡å¿—
            self.is_running = True
            
            print("\n" + "="*60)
            print("ğŸ‰ OpenAI Realtime API å®¢æˆ·ç«¯å·²å¯åŠ¨")
            print("ğŸ™ï¸  è¯·å¯¹ç€éº¦å…‹é£è¯´è¯ï¼ŒAI ä¼šå®æ—¶å›å¤")
            print("ğŸµ å£°éŸ³: shimmerï¼ˆæ¸©æš–å‹å¥½çš„å¥³å£°ï¼‰")
            print("ğŸŒ å¤šè¯­è¨€æ¨¡å¼ï¼šä¸­æ–‡/è‹±è¯­/æ—¥è¯­ç­‰è‡ªåŠ¨åˆ‡æ¢")
            print("âš¡ æ”¯æŒæ‰“æ–­ï¼šåœ¨ AI è¯´è¯æ—¶ï¼Œä½ å¯ä»¥éšæ—¶æ‰“æ–­å¹¶è¯´è¯")
            if ENABLE_ECHO_CANCELLATION:
                print(f"ğŸ”‡ å›éŸ³æ¶ˆé™¤å·²å¯ç”¨ï¼ˆæ‰“æ–­é˜ˆå€¼: {INTERRUPT_THRESHOLD}ï¼‰")
            if PLAYBACK_SPEED != 1.0:
                print(f"âš¡ æ’­æ”¾é€Ÿåº¦: {PLAYBACK_SPEED}x")
            if ENABLE_GAIN_CONTROL or ENABLE_NOISE_GATE:
                features = []
                if ENABLE_NOISE_GATE:
                    features.append("é™å™ª")
                if ENABLE_GAIN_CONTROL:
                    features.append("å¢ç›Š")
                print(f"ğŸ›ï¸  éŸ³é¢‘é¢„å¤„ç†: {', '.join(features)}")
            print("ğŸ’¡ æç¤ºï¼šä½¿ç”¨è€³æœºæ•ˆæœæœ€ä½³ï¼Œå¯é¿å…å›éŸ³")
            print("ğŸ›‘ æŒ‰ Ctrl+C é€€å‡º")
            print("="*60 + "\n")
            
            # å¹¶å‘è¿è¡ŒéŸ³é¢‘è¾“å…¥å’Œæ¶ˆæ¯å¤„ç†
            await asyncio.gather(
                self.start_audio_input(),
                self.handle_messages()
            )
            
        except KeyboardInterrupt:
            print("\n\nğŸ‘‹ æ­£åœ¨é€€å‡º...")
        except Exception as e:
            print(f"âŒ è¿è¡Œé”™è¯¯: {e}")
        finally:
            await self.cleanup()
            
    async def cleanup(self):
        """æ¸…ç†èµ„æº"""
        self.is_running = False
        
        if self.input_stream:
            self.input_stream.stop_stream()
            self.input_stream.close()
            
        if self.output_stream:
            self.output_stream.stop_stream()
            self.output_stream.close()
            
        if self.audio:
            self.audio.terminate()
            
        if self.ws:
            await self.ws.close()
            
        print("âœ… èµ„æºå·²æ¸…ç†")

def signal_handler(sig, frame):
    """å¤„ç† Ctrl+C ä¿¡å·"""
    print("\n\nğŸ‘‹ æ”¶åˆ°é€€å‡ºä¿¡å·...")
    sys.exit(0)

async def main():
    """ä¸»å‡½æ•°"""
    # æ£€æŸ¥ API Key
    if not OPENAI_API_KEY:
        print("âŒ é”™è¯¯: æœªæ‰¾åˆ° OPENAI_API_KEY")
        print("è¯·åœ¨ .env æ–‡ä»¶ä¸­è®¾ç½®ä½ çš„ API Key")
        return
    
    # æ³¨å†Œä¿¡å·å¤„ç†
    signal.signal(signal.SIGINT, signal_handler)
    
    # åˆ›å»ºå¹¶è¿è¡Œå®¢æˆ·ç«¯
    client = RealtimeClient()
    await client.run()

if __name__ == "__main__":
    asyncio.run(main())

