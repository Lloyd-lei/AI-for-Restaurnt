#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
OpenAI Realtime å®¢æˆ·ç«¯ - æœ¬åœ° Whisper ASR + OpenAI TTS
"""

import asyncio
import websockets
import json
import pyaudio
import numpy as np
import os
from dotenv import load_dotenv
import signal
import sys
import torch
import whisper
import time
import base64
from function_tools import FUNCTION_DEFINITIONS, execute_function
import threading

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

# ============================================================
# ğŸ¯ é…ç½®æ¥å£ - åœ¨è¿™é‡Œä¿®æ”¹è®¾ç½®
# ============================================================

# OpenAI é…ç½®
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
REALTIME_API_URL = "wss://api.openai.com/v1/realtime"
MODEL = "gpt-4o-realtime-preview-2024-10-01"

# ğŸ”§ Whisper æ¨¡å‹é€‰æ‹©æ¥å£
WHISPER_MODEL = "small"  # ğŸ¯ å¯é€‰: tiny, base, small, medium, large
# tiny   - æœ€å¿«ï¼Œå‡†ç¡®ç‡ä½
# base   - é€Ÿåº¦å’Œå‡†ç¡®ç‡å¹³è¡¡
# small  - è¾ƒå‡†ç¡®ï¼ˆæ¨èï¼‰âœ…
# medium - å¾ˆå‡†ç¡®ä½†æ…¢
# large  - æœ€å‡†ç¡®ä½†å¾ˆæ…¢

# éŸ³é¢‘å‚æ•°
SAMPLE_RATE = 16000  # Whisper è¦æ±‚ 16kHz
CHANNELS = 1
CHUNK_SIZE = 1024
FORMAT = pyaudio.paInt16

# VAD å‚æ•°ï¼ˆè¯­éŸ³æ´»åŠ¨æ£€æµ‹ï¼‰
ENERGY_THRESHOLD = 300  # ğŸ¯ è¯­éŸ³èƒ½é‡é˜ˆå€¼ï¼ˆ100-1000ï¼‰ï¼Œè¶Šå°è¶Šæ•æ„Ÿ
SILENCE_DURATION = 1.0  # ğŸ¯ é™éŸ³å¤šä¹…åç»“æŸï¼ˆç§’ï¼‰ï¼Œå»ºè®® 0.8-2.0
MIN_SPEECH_DURATION = 0.3  # æœ€å°è¯­éŸ³æ—¶é•¿ï¼ˆç§’ï¼‰

# TTS å‚æ•°
TTS_VOICE = "shimmer"  # ğŸ¯ å¯é€‰: alloy, echo, fable, onyx, nova, shimmer
PLAYBACK_SPEED = 1.0  # ğŸ¯ æ’­æ”¾é€Ÿåº¦ï¼ˆ1.0 = æ­£å¸¸ï¼Œ1.2 = 1.2å€é€Ÿï¼‰

# ============================================================

class RealtimeLocalASR:
    def __init__(self):
        self.ws = None
        self.audio = pyaudio.PyAudio()
        self.input_stream = None
        self.output_stream = None
        self.is_running = False
        self.is_ai_speaking = False
        self.session_id = None
        
        # Whisper
        self.whisper_model = None
        self.device = "cpu"
        
        # VAD
        self.speech_buffer = []
        self.is_speaking = False
        self.silence_start = None
        
        # æ‰“æ–­æ§åˆ¶
        self.interrupt_flag = False
        self.drop_audio_until_cancelled = False  # ä¸¢å¼ƒéŸ³é¢‘å¸§æ ‡å¿—
        
    def load_whisper(self):
        """åŠ è½½ Whisper æ¨¡å‹"""
        print(f"ğŸ”„ æ­£åœ¨åŠ è½½ Whisper æ¨¡å‹: {WHISPER_MODEL}")
        start_time = time.time()
        
        self.whisper_model = whisper.load_model(WHISPER_MODEL, device=self.device)
        
        load_time = time.time() - start_time
        print(f"âœ… Whisper æ¨¡å‹åŠ è½½å®Œæˆï¼ˆè€—æ—¶: {load_time:.2f}ç§’ï¼‰")
        
    def calculate_energy(self, audio_data):
        """è®¡ç®—éŸ³é¢‘èƒ½é‡"""
        audio_array = np.frombuffer(audio_data, dtype=np.int16)
        if len(audio_array) == 0:
            return 0
        energy = np.mean(audio_array.astype(np.float32) ** 2)
        return np.sqrt(energy) if energy > 0 else 0
    
    async def connect(self):
        """è¿æ¥åˆ° OpenAI Realtime API"""
        headers = {
            "Authorization": f"Bearer {OPENAI_API_KEY}",
            "OpenAI-Beta": "realtime=v1"
        }
        
        url = f"{REALTIME_API_URL}?model={MODEL}"
        
        print("ğŸ”„ æ­£åœ¨è¿æ¥åˆ° OpenAI Realtime API...")
        self.ws = await websockets.connect(url, extra_headers=headers)
        print("âœ… å·²è¿æ¥åˆ° OpenAI Realtime API")
        
        await self.configure_session()
        
    async def configure_session(self):
        """é…ç½®ä¼šè¯å‚æ•°"""
        config = {
            "type": "session.update",
            "session": {
                "modalities": ["text", "audio"],
                "instructions": (
                    "ä½ æ˜¯ä¸€ä¸ªå‹å¥½çš„å¤šè¯­è¨€AIåŠ©æ‰‹ï¼Œåå«å°åŠ©æ‰‹ã€‚ä½ å¯ä»¥ï¼š\n"
                    "1. ç”¨æˆ·è¯´ä»€ä¹ˆè¯­è¨€ï¼Œä½ å°±ç”¨ä»€ä¹ˆè¯­è¨€å›å¤\n"
                    "2. æŸ¥è¯¢å¤©æ°”ä¿¡æ¯\n"
                    "3. æ¨èé¾™å‡¤æ¥¼ä¸­é¤å…çš„ç¾é£Ÿ\n"
                    "4. æœç´¢å’Œæ¨èä¹¦ç±\n"
                    "5. å½“ç”¨æˆ·æ˜ç¡®è¡¨ç¤ºè¦ç»“æŸå¯¹è¯æ—¶ï¼Œè°ƒç”¨ end_conversation å‡½æ•°\n\n"
                    "å›å¤é£æ ¼ï¼šç®€æ´ã€è‡ªç„¶ã€å‹å¥½ã€‚ä½¿ç”¨ function calling æ¥å¤„ç†å…·ä½“æŸ¥è¯¢ã€‚"
                ),
                "voice": TTS_VOICE,
                "output_audio_format": "pcm16",
                "turn_detection": None,  # å…³é—­æœåŠ¡å™¨ç«¯ VADï¼Œä½¿ç”¨æœ¬åœ° VAD
                "tools": FUNCTION_DEFINITIONS  # ğŸ¯ æ·»åŠ  function calling
            }
        }
        
        await self.ws.send(json.dumps(config))
        print(f"âš™ï¸  ä¼šè¯é…ç½®å·²å‘é€ï¼ˆTTS: {TTS_VOICE}ï¼ŒFunctions: {len(FUNCTION_DEFINITIONS)}ä¸ªï¼‰")
        
    async def start_audio_input(self):
        """å¯åŠ¨éº¦å…‹é£è¾“å…¥"""
        self.input_stream = self.audio.open(
            format=FORMAT,
            channels=CHANNELS,
            rate=SAMPLE_RATE,
            input=True,
            frames_per_buffer=CHUNK_SIZE
        )
        
        print("ğŸ™ï¸  éº¦å…‹é£å·²å¯åŠ¨ï¼Œå¼€å§‹ç›‘å¬...")
        
        while self.is_running:
            try:
                # è¯»å–éŸ³é¢‘
                audio_data = await asyncio.to_thread(
                    self.input_stream.read, CHUNK_SIZE, False
                )
                
                # åªåœ¨ AI ä¸è¯´è¯æ—¶å¤„ç†ç”¨æˆ·è¾“å…¥
                if not self.is_ai_speaking:
                    energy = self.calculate_energy(audio_data)
                    
                    # VAD: æ£€æµ‹è¯­éŸ³æ´»åŠ¨
                    if energy > ENERGY_THRESHOLD:
                        if not self.is_speaking:
                            print("ğŸ—£ï¸  æ£€æµ‹åˆ°è¯­éŸ³...")
                            self.is_speaking = True
                        
                        self.speech_buffer.append(audio_data)
                        self.silence_start = None
                    else:
                        if self.is_speaking:
                            if self.silence_start is None:
                                self.silence_start = time.time()
                            elif time.time() - self.silence_start > SILENCE_DURATION:
                                # é™éŸ³è¶³å¤Ÿé•¿ï¼Œè¯†åˆ«è¯­éŸ³
                                print("ğŸ”‡ è¯­éŸ³ç»“æŸï¼Œå¼€å§‹è¯†åˆ«...")
                                await self._process_speech()
                                self.speech_buffer = []
                                self.is_speaking = False
                                self.silence_start = None
                        else:
                            # æ”¶é›†èƒŒæ™¯éŸ³ï¼ˆç”¨äºæ›´å‡†ç¡®çš„ VADï¼‰
                            self.speech_buffer.append(audio_data)
                
                await asyncio.sleep(0.001)
                
            except Exception as e:
                if self.is_running:
                    print(f"âŒ éŸ³é¢‘è¾“å…¥é”™è¯¯: {e}")
                break
                
    async def _process_speech(self):
        """å¤„ç†å¹¶è¯†åˆ«è¯­éŸ³"""
        if len(self.speech_buffer) < int(SAMPLE_RATE * MIN_SPEECH_DURATION / CHUNK_SIZE):
            print("âš ï¸  è¯­éŸ³å¤ªçŸ­ï¼Œè·³è¿‡")
            return
        
        # åˆå¹¶éŸ³é¢‘
        audio_data = b''.join(self.speech_buffer)
        audio_array = np.frombuffer(audio_data, dtype=np.int16).astype(np.float32) / 32768.0
        
        print("ğŸ” æ­£åœ¨è¯†åˆ«...")
        recognize_start = time.time()
        
        try:
            # Whisper è¯†åˆ«
            result = await asyncio.to_thread(
                self.whisper_model.transcribe,
                audio_array,
                language=None,  # è‡ªåŠ¨æ£€æµ‹è¯­è¨€
                fp16=False,
                verbose=False
            )
            
            recognize_time = time.time() - recognize_start
            text = result["text"].strip()
            language = result["language"]
            
            if text:
                print(f"ğŸ“ ä½ è¯´: {text} [{language}] (è€—æ—¶: {recognize_time:.2f}ç§’)")
                # å‘é€æ–‡æœ¬ç»™ OpenAI
                await self._send_text_to_openai(text)
            else:
                print("âš ï¸  æœªè¯†åˆ«åˆ°å†…å®¹")
            
        except Exception as e:
            print(f"âŒ è¯†åˆ«é”™è¯¯: {e}")
    
    async def _send_text_to_openai(self, text):
        """å‘é€æ–‡æœ¬ç»™ OpenAI å¹¶è¯·æ±‚éŸ³é¢‘å›å¤"""
        # åˆ›å»ºå¯¹è¯é¡¹
        message = {
            "type": "conversation.item.create",
            "item": {
                "type": "message",
                "role": "user",
                "content": [
                    {
                        "type": "input_text",
                        "text": text
                    }
                ]
            }
        }
        await self.ws.send(json.dumps(message))
        
        # è¯·æ±‚å“åº”
        response_msg = {
            "type": "response.create",
            "response": {
                "modalities": ["audio", "text"]
            }
        }
        await self.ws.send(json.dumps(response_msg))
    
    async def _send_function_result(self, call_id, result):
        """å‘é€å‡½æ•°æ‰§è¡Œç»“æœç»™ OpenAI"""
        message = {
            "type": "conversation.item.create",
            "item": {
                "type": "function_call_output",
                "call_id": call_id,
                "output": json.dumps(result, ensure_ascii=False)
            }
        }
        await self.ws.send(json.dumps(message))
        
        # è¯·æ±‚ AI ç»§ç»­å“åº”
        response_msg = {
            "type": "response.create"
        }
        await self.ws.send(json.dumps(response_msg))
        
    async def start_audio_output(self):
        """å¯åŠ¨éŸ³é¢‘è¾“å‡º"""
        playback_rate = int(24000 * PLAYBACK_SPEED)  # OpenAI è¾“å‡ºæ˜¯ 24kHz
        
        self.output_stream = self.audio.open(
            format=FORMAT,
            channels=CHANNELS,
            rate=playback_rate,
            output=True,
            frames_per_buffer=4800
        )
        
        if PLAYBACK_SPEED != 1.0:
            print(f"ğŸ”Š éŸ³é¢‘è¾“å‡ºå·²å¯åŠ¨ï¼ˆ{PLAYBACK_SPEED}x å€é€Ÿï¼‰")
        else:
            print("ğŸ”Š éŸ³é¢‘è¾“å‡ºå·²å¯åŠ¨")
    
    async def keyboard_listener(self):
        """ç›‘å¬é”®ç›˜è¾“å…¥ï¼ˆæŒ‰å›è½¦æ‰“æ–­ï¼‰"""
        def listen_keyboard():
            while self.is_running:
                try:
                    input()  # ç­‰å¾…å›è½¦é”®
                    if self.is_ai_speaking:
                        self.interrupt_flag = True
                except:
                    break
        
        # åœ¨å•ç‹¬çš„çº¿ç¨‹ä¸­è¿è¡Œ
        thread = threading.Thread(target=listen_keyboard, daemon=True)
        thread.start()
        
        # æ£€æŸ¥æ‰“æ–­æ ‡å¿—
        while self.is_running:
            if self.interrupt_flag:
                print("\nâš¡ æ£€æµ‹åˆ°æ‰“æ–­ï¼ˆå›è½¦é”®ï¼‰ï¼Œç«‹å³åœæ­¢æ’­æ”¾")
                
                # ğŸ¯ 1. ç«‹å³è®¾ç½®æ ‡å¿—ï¼Œä¸¢å¼ƒåç»­éŸ³é¢‘å¸§ï¼ˆä¸ç­‰æœåŠ¡å™¨ç¡®è®¤ï¼‰
                self.drop_audio_until_cancelled = True
                self.is_ai_speaking = False
                
                # ğŸ¯ 2. ç«‹å³æ¸…ç©ºéŸ³é¢‘ç¼“å†²åŒºå¹¶é‡å¯æµ
                if self.output_stream:
                    try:
                        self.output_stream.stop_stream()
                        self.output_stream.close()
                        
                        # é‡æ–°åˆ›å»ºéŸ³é¢‘æµï¼ˆå½»åº•æ¸…ç©ºç¼“å†²åŒºï¼‰
                        playback_rate = int(24000 * PLAYBACK_SPEED)
                        self.output_stream = self.audio.open(
                            format=FORMAT,
                            channels=CHANNELS,
                            rate=playback_rate,
                            output=True,
                            frames_per_buffer=4800
                        )
                        print("ğŸ”‡ éŸ³é¢‘ç¼“å†²åŒºå·²æ¸…ç©º")
                    except Exception as e:
                        print(f"âš ï¸  é‡ç½®éŸ³é¢‘æµ: {e}")
                
                # ğŸ¯ 3. å¼‚æ­¥å‘é€å–æ¶ˆæ¶ˆæ¯åˆ°æœåŠ¡å™¨ï¼ˆä¸é˜»å¡ï¼‰
                try:
                    cancel_msg = {
                        "type": "response.cancel"
                    }
                    await self.ws.send(json.dumps(cancel_msg))
                    print("ğŸ“¤ å·²å‘é€å–æ¶ˆè¯·æ±‚åˆ°æœåŠ¡å™¨ï¼ˆç­‰å¾…ç¡®è®¤ï¼‰")
                except Exception as e:
                    print(f"âŒ å‘é€å–æ¶ˆè¯·æ±‚å¤±è´¥: {e}")
                    # å³ä½¿å‘é€å¤±è´¥ï¼Œæœ¬åœ°ä¹Ÿå·²ç»åœæ­¢äº†
                    self.drop_audio_until_cancelled = False
                
                self.interrupt_flag = False
                print("ğŸ™ï¸  å·²æ¢å¤ç›‘å¬ï¼Œå¯ä»¥ç»§ç»­è¯´è¯...")
            
            await asyncio.sleep(0.05)  # æ›´å¿«çš„æ£€æŸ¥é¢‘ç‡ï¼ˆ50msï¼‰
        
    async def handle_messages(self):
        """å¤„ç†æ¥è‡ªæœåŠ¡å™¨çš„æ¶ˆæ¯"""
        try:
            async for message in self.ws:
                data = json.loads(message)
                event_type = data.get("type")
                
                if event_type == "session.created":
                    self.session_id = data.get("session", {}).get("id")
                    print(f"ğŸ“ ä¼šè¯å·²åˆ›å»º: {self.session_id}")
                
                elif event_type == "session.updated":
                    print("âœ… ä¼šè¯é…ç½®å·²æ›´æ–°")
                
                elif event_type == "response.created":
                    print("ğŸ¤– AI å¼€å§‹ç”Ÿæˆå›å¤...")
                
                elif event_type == "response.done":
                    print("âœ… AI å›å¤å®Œæˆ\n")
                    self.is_ai_speaking = False
                
                elif event_type == "response.text.delta":
                    delta = data.get("delta", "")
                    print(delta, end="", flush=True)
                
                elif event_type == "response.text.done":
                    text = data.get("text", "")
                    if text:
                        print(f"\nğŸ’¬ AI: {text}")
                
                elif event_type == "response.audio.delta":
                    # ğŸ¯ å¦‚æœæ­£åœ¨ç­‰å¾…å–æ¶ˆç¡®è®¤ï¼Œä¸¢å¼ƒæ‰€æœ‰éŸ³é¢‘å¸§
                    if self.drop_audio_until_cancelled:
                        # é™é»˜ä¸¢å¼ƒï¼Œä¸æ’­æ”¾
                        continue
                    
                    self.is_ai_speaking = True
                    audio_b64 = data.get("delta", "")
                    if audio_b64 and self.output_stream:
                        audio_data = base64.b64decode(audio_b64)
                        try:
                            self.output_stream.write(audio_data)
                        except Exception as e:
                            # å¯èƒ½åœ¨é‡ç½®æµæ—¶å‡ºé”™ï¼Œå¿½ç•¥
                            pass
                
                elif event_type == "response.audio.done":
                    print("ğŸ”Š AI è¯­éŸ³æ’­æ”¾å®Œæˆ")
                    self.is_ai_speaking = False
                
                # ğŸ¯ Function call ç›¸å…³äº‹ä»¶
                elif event_type == "response.function_call_arguments.delta":
                    # Function å‚æ•°å¢é‡ï¼ˆå¯é€‰ï¼šæ˜¾ç¤ºå‚æ•°æ„å»ºè¿‡ç¨‹ï¼‰
                    pass
                
                elif event_type == "response.function_call_arguments.done":
                    # Function å‚æ•°æ¥æ”¶å®Œæˆ
                    call_id = data.get("call_id")
                    function_name = data.get("name")
                    arguments_str = data.get("arguments", "{}")
                    
                    print(f"\nğŸ”§ è°ƒç”¨å‡½æ•°: {function_name}")
                    print(f"ğŸ“‹ å‚æ•°: {arguments_str}")
                    
                    # æ‰§è¡Œå‡½æ•°
                    try:
                        arguments = json.loads(arguments_str)
                        result = execute_function(function_name, arguments)
                        
                        print(f"âœ… å‡½æ•°ç»“æœ: {json.dumps(result, ensure_ascii=False, indent=2)}")
                        
                        # ğŸ¯ ç‰¹æ®Šå¤„ç†ï¼šend_conversation
                        if function_name == "end_conversation":
                            print("\nğŸ‘‹ ç”¨æˆ·é€‰æ‹©ç»“æŸå¯¹è¯")
                            # å…ˆå‘é€å‡½æ•°ç»“æœï¼Œç„¶åé€€å‡º
                            await self._send_function_result(call_id, result)
                            await asyncio.sleep(2)  # ç­‰å¾… AI è¯´å®Œå†è§
                            self.is_running = False
                            return
                        
                        # å‘é€å‡½æ•°ç»“æœç»™ OpenAI
                        await self._send_function_result(call_id, result)
                        
                    except Exception as e:
                        print(f"âŒ å‡½æ•°æ‰§è¡Œå¤±è´¥: {e}")
                        error_result = {"error": str(e)}
                        await self._send_function_result(call_id, error_result)
                
                # ğŸ¯ å“åº”è¢«å–æ¶ˆï¼ˆæ‰“æ–­ï¼‰
                elif event_type == "response.cancelled":
                    print("âœ… æœåŠ¡å™¨ç¡®è®¤ï¼šå“åº”å·²å–æ¶ˆ")
                    self.is_ai_speaking = False
                    self.drop_audio_until_cancelled = False  # é‡ç½®ä¸¢å¼ƒæ ‡å¿—
                
                elif event_type == "error":
                    error = data.get("error", {})
                    print(f"âŒ é”™è¯¯: {error.get('message', 'æœªçŸ¥é”™è¯¯')}")
                
        except websockets.exceptions.ConnectionClosed:
            print("ğŸ”Œ è¿æ¥å·²å…³é—­")
        except Exception as e:
            print(f"âŒ æ¶ˆæ¯å¤„ç†é”™è¯¯: {e}")
            
    async def run(self):
        """è¿è¡Œå®¢æˆ·ç«¯"""
        try:
            # åŠ è½½ Whisper
            self.load_whisper()
            
            # è¿æ¥æœåŠ¡å™¨
            await self.connect()
            
            # å¯åŠ¨éŸ³é¢‘è¾“å‡º
            await self.start_audio_output()
            
            self.is_running = True
            
            print("\n" + "="*60)
            print("ğŸ‰ Realtime å®¢æˆ·ç«¯å·²å¯åŠ¨ï¼ˆæœ¬åœ° ASR + Function Callingï¼‰")
            print(f"ğŸ™ï¸  æœ¬åœ° ASR: Whisper {WHISPER_MODEL.upper()}")
            print(f"ğŸ—£ï¸  OpenAI TTS: {TTS_VOICE}")
            print(f"ğŸšï¸  VAD é˜ˆå€¼: {ENERGY_THRESHOLD}")
            print(f"â±ï¸  é™éŸ³æ—¶é•¿: {SILENCE_DURATION}ç§’")
            print("ğŸŒ å¤šè¯­è¨€è‡ªåŠ¨è¯†åˆ«")
            print("\nğŸ”§ å¯ç”¨åŠŸèƒ½:")
            print("  ğŸ“ æŸ¥å¤©æ°” - é—®ã€ŒåŒ—äº¬ä»Šå¤©å¤©æ°”æ€ä¹ˆæ ·ï¼Ÿã€")
            print("  ğŸœ æŸ¥èœå• - é—®ã€Œæœ‰ä»€ä¹ˆæ¨èçš„èœï¼Ÿã€ã€Œæœ‰ä¸è¾£çš„èœå—ï¼Ÿã€")
            print("  ğŸ“š æœä¹¦ç± - é—®ã€Œæ¨èç§‘å¹»å°è¯´ã€ã€Œåˆ˜æ…ˆæ¬£çš„ä¹¦ã€")
            print("  ğŸ‘‹ ç»“æŸå¯¹è¯ - è¯´ã€Œå†è§ã€ã€Œæ‹œæ‹œã€")
            print("\nğŸ’¡ æ“ä½œæç¤º:")
            print("  ğŸ¤ è¯´è¯ååœé¡¿å³å¯è‡ªåŠ¨è¯†åˆ«")
            print("  âš¡ æŒ‰å›è½¦é”®å¯ä»¥å–æ¶ˆ AI å“åº”ï¼ˆåœæ­¢ LLM+TTS ç”Ÿæˆï¼‰")
            print("  ğŸ›‘ æŒ‰ Ctrl+C å¼ºåˆ¶é€€å‡º")
            print("="*60 + "\n")
            
            # è¿è¡Œï¼ˆæ·»åŠ é”®ç›˜ç›‘å¬ï¼‰
            await asyncio.gather(
                self.start_audio_input(),
                self.handle_messages(),
                self.keyboard_listener()
            )
            
        except KeyboardInterrupt:
            print("\n\nğŸ‘‹ æ­£åœ¨é€€å‡º...")
        except Exception as e:
            print(f"âŒ è¿è¡Œé”™è¯¯: {e}")
            import traceback
            traceback.print_exc()
        finally:
            await self.cleanup()
            
    async def cleanup(self):
        """æ¸…ç†èµ„æº"""
        self.is_running = False
        
        if self.input_stream:
            self.input_stream.stop_stream()
            self.input_stream.close()
            
        if self.output_stream:
            self.output_stream.stop_stream()
            self.output_stream.close()
            
        if self.audio:
            self.audio.terminate()
            
        if self.ws:
            await self.ws.close()
            
        print("âœ… èµ„æºå·²æ¸…ç†")

def signal_handler(sig, frame):
    """å¤„ç† Ctrl+C ä¿¡å·"""
