#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç®€å•çš„ Whisper æµ‹è¯•è„šæœ¬
æµ‹è¯•ä¸åŒæ¨¡å‹å¤§å°çš„é€Ÿåº¦å’Œå‡†ç¡®åº¦
"""

import whisper
import pyaudio
import numpy as np
import time
import wave

# éŸ³é¢‘å‚æ•°
SAMPLE_RATE = 16000
CHANNELS = 1
CHUNK_SIZE = 1024
FORMAT = pyaudio.paInt16
RECORD_SECONDS = 10  # å½•éŸ³æ—¶é•¿

def record_audio(duration=5):
    """å½•åˆ¶éŸ³é¢‘"""
    print(f"\nğŸ™ï¸  å¼€å§‹å½•éŸ³ï¼ˆ{duration}ç§’ï¼Œçº¦{duration/60:.1f}åˆ†é’Ÿï¼‰...")
    print("ğŸ’¬ è¯·è¯´è¯...")
    
    audio = pyaudio.PyAudio()
    
    stream = audio.open(
        format=FORMAT,
        channels=CHANNELS,
        rate=SAMPLE_RATE,
        input=True,
        frames_per_buffer=CHUNK_SIZE
    )
    
    frames = []
    total_chunks = int(SAMPLE_RATE / CHUNK_SIZE * duration)
    
    # æ˜¾ç¤ºè¿›åº¦
    import sys
    for i in range(total_chunks):
        data = stream.read(CHUNK_SIZE, exception_on_overflow=False)
        frames.append(data)
        
        # æ¯ç§’æ˜¾ç¤ºä¸€æ¬¡è¿›åº¦
        if i % (SAMPLE_RATE // CHUNK_SIZE) == 0:
            elapsed = i / (SAMPLE_RATE / CHUNK_SIZE)
            progress = (elapsed / duration) * 100
            sys.stdout.write(f"\râ±ï¸  å½•éŸ³è¿›åº¦: {elapsed:.0f}/{duration}ç§’ ({progress:.1f}%)")
            sys.stdout.flush()
    
    print("\nâœ… å½•éŸ³å®Œæˆï¼")
    
    stream.stop_stream()
    stream.close()
    audio.terminate()
    
    # è½¬æ¢ä¸º numpy æ•°ç»„
    audio_data = b''.join(frames)
    audio_array = np.frombuffer(audio_data, dtype=np.int16).astype(np.float32) / 32768.0
    
    return audio_array

def test_model(model_name, audio_data):
    """æµ‹è¯•å•ä¸ªæ¨¡å‹"""
    print(f"\n{'='*60}")
    print(f"ğŸ” æµ‹è¯•æ¨¡å‹: {model_name}")
    print(f"{'='*60}")
    
    # åŠ è½½æ¨¡å‹
    print("â³ æ­£åœ¨åŠ è½½æ¨¡å‹...")
    load_start = time.time()
    model = whisper.load_model(model_name)
    load_time = time.time() - load_start
    print(f"âœ… æ¨¡å‹åŠ è½½å®Œæˆï¼ˆè€—æ—¶: {load_time:.2f}ç§’ï¼‰")
    
    # è¯†åˆ«
    print("ğŸ” æ­£åœ¨è¯†åˆ«...")
    transcribe_start = time.time()
    result = model.transcribe(
        audio_data,
        language=None,  # è‡ªåŠ¨æ£€æµ‹è¯­è¨€
        fp16=False,
        verbose=False
    )
    transcribe_time = time.time() - transcribe_start
    
    # è¾“å‡ºç»“æœ
    text = result["text"].strip()
    language = result["language"]
    
    print(f"\nğŸ“ è¯†åˆ«ç»“æœ: {text}")
    print(f"ğŸŒ æ£€æµ‹è¯­è¨€: {language}")
    print(f"â±ï¸  è¯†åˆ«è€—æ—¶: {transcribe_time:.2f}ç§’")
    print(f"ğŸ“Š æ¨¡å‹å¤§å°: {model_name}")
    
    return {
        "model": model_name,
        "text": text,
        "language": language,
        "load_time": load_time,
        "transcribe_time": transcribe_time
    }

def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description="æµ‹è¯• Whisper æ¨¡å‹")
    parser.add_argument(
        "--models",
        type=str,
        nargs="+",
        default=["tiny", "base"],
        choices=["tiny", "base", "small", "medium", "large", "all"],
        help="è¦æµ‹è¯•çš„æ¨¡å‹ï¼ˆå¯ä»¥æŒ‡å®šå¤šä¸ªï¼Œæˆ–ä½¿ç”¨ 'all' æµ‹è¯•æ‰€æœ‰æ¨¡å‹ï¼‰"
    )
    parser.add_argument(
        "--duration",
        type=int,
        default=5,
        help="å½•éŸ³æ—¶é•¿ï¼ˆç§’ï¼‰ï¼Œé»˜è®¤5ç§’ã€‚20åˆ†é’Ÿ=1200ç§’"
    )
    
    args = parser.parse_args()
    
    # å¤„ç† 'all' é€‰é¡¹
    if 'all' in args.models:
        models_to_test = ["tiny", "base", "small", "medium", "large"]
    else:
        models_to_test = args.models
    
    print("ğŸ¤ Whisper æ¨¡å‹æµ‹è¯•å·¥å…·")
    print(f"ğŸ“¦ å°†æµ‹è¯•æ¨¡å‹: {', '.join(models_to_test)}")
    print(f"â±ï¸  å½•éŸ³æ—¶é•¿: {args.duration}ç§’ï¼ˆçº¦{args.duration/60:.1f}åˆ†é’Ÿï¼‰")
    
    # ä¼°ç®—æ—¶é—´
    total_time_estimate = len(models_to_test) * 5 + args.duration
    print(f"â° é¢„è®¡æ€»è€—æ—¶: çº¦{total_time_estimate/60:.1f}åˆ†é’Ÿ")
    
    print("\nğŸ’¡ æ¨¡å‹è¯´æ˜:")
    print("  - tiny: æœ€å¿«ï¼Œå‡†ç¡®ç‡è¾ƒä½ï¼ˆ~39Mï¼‰")
    print("  - base: æ¨èï¼Œé€Ÿåº¦å’Œå‡†ç¡®ç‡å¹³è¡¡ï¼ˆ~74Mï¼‰")
    print("  - small: è¾ƒå‡†ç¡®ï¼ˆ~244Mï¼‰")
    print("  - medium: å¾ˆå‡†ç¡®ï¼ˆ~769Mï¼‰")
    print("  - large: æœ€å‡†ç¡®ï¼Œæœ€æ…¢ï¼ˆ~1550Mï¼‰")
    
    if args.duration >= 60:
        print(f"\nâš ï¸  æ³¨æ„: å½•éŸ³æ—¶é•¿è¾ƒé•¿ï¼ˆ{args.duration/60:.1f}åˆ†é’Ÿï¼‰ï¼Œè¯·ç¡®ä¿:")
        print("   1. éº¦å…‹é£æ­£å¸¸å·¥ä½œ")
        print("   2. æœ‰è¶³å¤Ÿçš„å­˜å‚¨ç©ºé—´")
        print("   3. å‡†å¤‡å¥½æµ‹è¯•å†…å®¹")
        input("\næŒ‰ Enter é”®å¼€å§‹å½•éŸ³...")
    
    # å½•éŸ³
    audio_data = record_audio(args.duration)
    
    # ä¿å­˜éŸ³é¢‘æ–‡ä»¶ï¼ˆå¯é€‰ï¼‰
    print("\nğŸ’¾ æ­£åœ¨ä¿å­˜å½•éŸ³æ–‡ä»¶...")
    import wave
    with wave.open('test_recording.wav', 'wb') as wf:
        wf.setnchannels(CHANNELS)
        wf.setsampwidth(2)  # 16-bit
        wf.setframerate(SAMPLE_RATE)
        audio_bytes = (audio_data * 32768).astype(np.int16).tobytes()
        wf.writeframes(audio_bytes)
    print("âœ… å½•éŸ³å·²ä¿å­˜ä¸º: test_recording.wav")
    
    # æµ‹è¯•æ‰€æœ‰æ¨¡å‹
    results = []
    for idx, model_name in enumerate(models_to_test, 1):
        print(f"\n{'='*60}")
        print(f"ğŸ“Š è¿›åº¦: {idx}/{len(models_to_test)}")
        print(f"{'='*60}")
        try:
            result = test_model(model_name, audio_data)
            results.append(result)
        except Exception as e:
            print(f"âŒ æ¨¡å‹ {model_name} æµ‹è¯•å¤±è´¥: {e}")
    
    # æ±‡æ€»ç»“æœ
    if len(results) > 1:
        print(f"\n{'='*60}")
        print("ğŸ“Š æµ‹è¯•ç»“æœå¯¹æ¯”")
        print(f"{'='*60}")
        
        for r in results:
            print(f"\nğŸ”¹ {r['model'].upper()}")
            print(f"   è¯†åˆ«: {r['text']}")
            print(f"   è¯­è¨€: {r['language']}")
            print(f"   åŠ è½½: {r['load_time']:.2f}ç§’")
            print(f"   è¯†åˆ«: {r['transcribe_time']:.2f}ç§’")
        
        print(f"\n{'='*60}")
        print("ğŸ’¡ å»ºè®®:")
        print("  - å¦‚æœè¿½æ±‚é€Ÿåº¦: ä½¿ç”¨ tiny æˆ– base")
        print("  - å¦‚æœè¿½æ±‚å‡†ç¡®åº¦: ä½¿ç”¨ small æˆ– medium")
        print("  - å®æ—¶åº”ç”¨æ¨è: baseï¼ˆæœ€ä½³å¹³è¡¡ï¼‰")

if __name__ == "__main__":
    main()

